Just a hidden of list things that I think has potential to be explored about.





# Extending Convolution

Basics on convlution:

* AlexNet, Convolution, CNN: https://www.youtube.com/watch?v=UZDiGooFs54
* https://github.com/vdumoulin/conv_arithmetic
* https://en.wikipedia.org/wiki/Convolution_theorem

Note that the similarity of convolution with cellular automata (CA). So one way to explore is to see what other exciting stuff that people have done with CA:

* Basics on CA: https://en.wikipedia.org/wiki/Cellular_automaton and the underlying theory of https://en.wikipedia.org/wiki/Automata_theory . CA is big deal because it relates to the underlying theory of computation in general https://en.wikipedia.org/wiki/Abstract_machine and even more broadly: https://en.wikipedia.org/wiki/Artificial_life which goes into full circle back to AI.
* MNCA: multiple neighborhoods cellular automata. The authoritative figure for this is Slackermanz: https://slackermanz.com/ . Check the YouTube https://www.youtube.com/watch?v=xyx5C40HpQM 
* Lenia: CA but continous instead of discrete https://arxiv.org/abs/2005.03742 https://www.youtube.com/watch?v=7-97RhAZhXI
* Particle Life: [reformulation of CA](https://youtu.be/Z_zmZ23grXE?si=DUSOc9f8xnDHB6Rt) using [n-body](https://en.wikipedia.org/wiki/N-body_problem#Other_n-body_problems) https://www.ventrella.com/Clusters/ and a YouTube viz and explainer https://youtu.be/p4YirERTVF0?si=32_mmGty--kFmSw6 that includes demo and implementation https://particle-life.com/ 

So there are two ways to do this.
1. We can use CA as a way to add mechanistic interpretability into existing CNN.
2. The other way is to use CNN to figure out the CA rules required to get the behaviour that we want.




# Rendering density / distribution / energy

Some claims that the heart of DL is simply a a probability density (or distribution, or energy function) estimation problem.

It seems that neural rendering and radiance field are really good at these tasks.

Why not use them to estimate the distribution directly?




# XAI for TS
XAI: eXplainable Artificial Intelligence
TS: timeseries 

There are many ways to explain a TS.
Many of them are domain specific, like candle sticks in technical analysis, or the PQRSTU waves in electrocargiogram.
We are not interested in those.
We are interested in the domain agnostic way.

There are many domain agnostic ways to explain / decompose TS:
* Fourier Analysis & other spectral methods: This approach assumes that the time series can be represented as a sum of sinusoidal waves, implying that the data is generated by a process with underlying periodicities or frequency components.
* Trend, Seasonal and Residual
* Shape based methods
* Markovian / closeness / auto-correlation / implicit function / ODE
* Stochastic process

Choosing which decomposition technique (or the combination of the techniques) depends on the assumptions we make regarding the TS data generation process.
This goes back manifold hypothesis.
There is a data generating TS functional (g()) with a small number of arguments (a_1, a_2, a_3, ..., a_p) that produce a family of TS.
This functional generate a univariate TS is an infinitely long and dense real scalar function of time.
The data that we are working with is a finite sample of from this function with a finite length (l).
p << l.
Thus, a TS can be fully described by g(a).
For XAI, we are limiting the choice of g() to the ones that human can understand.

The goal for XAI for TS is to find g(a) given a TS.

The choice of g() express the underlying assumption.
* **Fourier Analysis & other spectral methods:** This approach assumes that the time series can be represented as a sum of sinusoidal waves, implying that the data is generated by a process with underlying periodicities or frequency components. E.g.: weather data.
* **Trend, Seasonal and Residual:** This method assumes that the time series is composed of distinct, separable components: a long-term trend, a repeating seasonal pattern, and random noise, suggesting an additive or multiplicative combination of these independent generating factors. E.g.: traffic, sales of certain products.
* **Shape based methods:** These methods assume that the shape or pattern of subsequences within the time series is more informative than the exact values, implying that the data generation process produces characteristic shapes that are relevant for analysis. E.g.: speech, trajectories.
* **Markovian / closeness / auto-correlation / implicit function / ODE:** These approaches assume that the current value of the time series is dependent on its past values, either directly through autoregressive relationships or indirectly through underlying dynamic systems described by functions or differential equations. E.g.: orbits, pendulum, prey and predator.
* **Stochastic process:** This approach assumes that the time series is a realization of a random process, implying that the data is generated by a probabilistic model with inherent uncertainty and dependencies. E.g.: [The random walk hypothesis of stock market](https://en.wikipedia.org/wiki/Random_walk_hypothesis).


